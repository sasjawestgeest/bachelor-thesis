{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle5 as pickle55\n",
    "\n",
    "import os\n",
    "\n",
    "from query_phenomizer import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Update_HPO:\n",
    "    \n",
    "    def __init__(self, obo_file):\n",
    "        self.obo_file = obo_file\n",
    "        self.replacement_dict = self.create_dictionary_replacements()\n",
    "        self.non_phenotype_nodes = self.find_non_phenotype_nodes()\n",
    "   \n",
    "    def find_non_phenotype_nodes(self):\n",
    "        non_phenotype_nodes = set(['HP:0000005', 'HP:0012823', 'HP:0040279'])\n",
    "        \n",
    "        nodes_added = len(non_phenotype_nodes)\n",
    "        while nodes_added > 0:\n",
    "\n",
    "            hpobo = open(self.obo_file)\n",
    "            nodes_added = 0\n",
    "            term = ''\n",
    "\n",
    "            for line in hpobo:\n",
    "                if line.startswith('id'):\n",
    "                    term = line.split(': ')[1].split('\\n')[0]\n",
    "\n",
    "                elif line.startswith('is_a'):\n",
    "                    hpo_term = line.split(': ')[1].split(' !')[0]\n",
    "                    if hpo_term in non_phenotype_nodes and term not in non_phenotype_nodes:\n",
    "                        non_phenotype_nodes.add(term)\n",
    "                        nodes_added += 1\n",
    "        return non_phenotype_nodes\n",
    "\n",
    "\n",
    "    def create_dictionary_replacements(self):\n",
    "        hpobo = open(self.obo_file)\n",
    "        replacements = {} #key is replaced by value\n",
    "\n",
    "        term = ''\n",
    "\n",
    "        for line in hpobo:\n",
    "            if line.startswith('id'):\n",
    "                term = line.split(': ')[1].split('\\n')[0]\n",
    "\n",
    "            elif line.startswith('replaced_by'):\n",
    "                hpo_term = line.split(': ')[1].split('\\n')[0]\n",
    "                replacements[term] = hpo_term\n",
    "\n",
    "            elif line.startswith('alt_id'):\n",
    "                hpo_term = line.split(': ')[1].split('\\n')[0]\n",
    "                replacements[hpo_term] = term\n",
    "        return replacements\n",
    "\n",
    "    \n",
    "    def create_dictionary_id_name(self): #gebruik ik helemaal niet\n",
    "        hpobo = open(self.obo_file)\n",
    "        id_to_name = {}\n",
    "\n",
    "        term_id = ''\n",
    "\n",
    "        for line in hpobo:\n",
    "            if line.startswith('id'):\n",
    "                term_id = line.split(': ')[1].split('\\n')[0]\n",
    "\n",
    "            elif line.startswith('name'):\n",
    "                term_name = line.split(': ')[1].split('\\n')[0]\n",
    "                id_to_name[term_id] = term_name\n",
    "\n",
    "        return id_to_name\n",
    "    \n",
    "    def replace(self, term):\n",
    "        if term in self.replacement_dict.keys():\n",
    "            return self.replacement_dict[term]\n",
    "        else:\n",
    "            return term\n",
    "\n",
    "    \n",
    "    def delete_non_phenotype_nodes(self, term_list):\n",
    "        new_term_list = [i for i in term_list if i not in self.non_phenotype_nodes]\n",
    "        return new_term_list\n",
    "\n",
    "\n",
    "    def update_phenotype(self, patient):\n",
    "        replaced = [self.replace(term) for term in patient]\n",
    "        replaced_deleted = self.delete_non_phenotype_nodes(replaced)\n",
    "        return replaced_deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "\n",
    "file = './data_all.pickle'  \n",
    "with open(file, \"rb\") as fh:\n",
    "      data = pickle55.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "phenopy_data_directory = os.path.join(os.getenv('HOME'), '.phenopy/data')\n",
    "obo_file = os.path.join(phenopy_data_directory, 'hp.obo')\n",
    "hpoa_file = os.path.join(phenopy_data_directory, 'phenotype.hpoa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "updater = Update_HPO(obo_file)\n",
    "for index in range(len(data['hpo_all'])):\n",
    "    data['hpo_all'][index] = updater.update_phenotype(data['hpo_all'][index])\n",
    "    data['hpo_all_with_parents'][index] = updater.update_phenotype(data['hpo_all_with_parents'][index])\n",
    "    data['label'][index] = data['label'][index].replace(\"_\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "omim_genes = {\n",
    "    'OMIM:148050' : 'ankrd',\n",
    "    'OMIM:300958' : 'ddx3x',\n",
    "    'OMIM:610443' : 'kansl1',\n",
    "    'OMIM:611867' : '22q11',\n",
    "    'OMIM:614104' : 'dyrk1a',\n",
    "    'OMIM:615009' : 'pacs1',\n",
    "    'OMIM:615873' : 'adnp',\n",
    "    'OMIM:616158' : 'pura',\n",
    "    'OMIM:616708' : 'wac',\n",
    "    'OMIM:617140' : 'son',\n",
    "    'OMIM:618846' : 'kdm3b', \n",
    "    'OMIM:618829' : 'spop_2',\n",
    "    'OMIM:618828' : 'spop_1',\n",
    "    'OMIM:617854' : 'cltc',\n",
    "    'OMIM:617557' : 'yy1',\n",
    "    'OMIM:617450' : 'ppm1d'\n",
    "}\n",
    "\n",
    "diseases = list(omim_genes.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPOA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of OMIM disease ID's found in HPOA file: 8120\n",
      "Amount of (OMIM/ORPHA/DICEPHER) disease ID's found in HPO file: 12367\n"
     ]
    }
   ],
   "source": [
    "hpoa = open(hpoa_file)\n",
    "omims = []\n",
    "ids = []\n",
    "\n",
    "for line in hpoa:\n",
    "    if line.startswith('OMIM') or line.startswith('ORPHA') or line.startswith(\"DECIPHER\"):\n",
    "        disease_id = line.split('\\t')[0]\n",
    "        ids.append(disease_id)\n",
    "    if line.startswith('OMIM'):\n",
    "        omims.append(disease_id)\n",
    "    \n",
    "print(\"Amount of OMIM disease ID's found in HPOA file:\",len(set(omims)))\n",
    "print(\"Amount of (OMIM/ORPHA/DICEPHER) disease ID's found in HPO file:\", len(set(ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 100\n",
    "username = 'fill_in_yourself'\n",
    "password = 'fill_in_yourself'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gene_predictions = {}\n",
    "for index in range(len(data['hpo_all'])):\n",
    "    separator = ', '\n",
    "    a = (separator.join(data['hpo_all'][index])) \n",
    "\n",
    "    result = utils.query_phenomizer(username, password,  a)\n",
    "    genes = []\n",
    "    for i in range(top_n):\n",
    "        genes.append(result.text.split('\\n')[6:][i].split('\\t')[4])\n",
    "    \n",
    "    seperate_genes = [j.split(' ')[0] for j in genes]\n",
    "    without_empty = list(filter(None, seperate_genes))\n",
    "    final_genes = [gene.lower() for gene in without_empty]\n",
    "    gene_predictions[index] = final_genes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OMIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnoses_database = {}\n",
    "for index in range(len(data['hpo_all'])):\n",
    "    separator = ', '\n",
    "    a = (separator.join(data['hpo_all'][index])) \n",
    "\n",
    "    result = utils.query_phenomizer('scout', 'scout123',  a)\n",
    "\n",
    "    diagnoses_omim = []\n",
    "    for i in range(top_n):\n",
    "        omim = result.text.split('\\n')[6:][i].split('\\t')[2]\n",
    "        diagnoses_omim.append(omim)\n",
    "\n",
    "    diagnoses_database[index] = diagnoses_omim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct labels found in top 100: 23\n",
      "Accuracy: 0.04159132007233273\n"
     ]
    }
   ],
   "source": [
    "correct_label_in_top_n = 0\n",
    "\n",
    "for key in gene_predictions:\n",
    "    genes = set(gene_predictions[key])\n",
    "    diagnoses = diagnoses_database[key]\n",
    "    \n",
    "    if data['label'][key] in gene_predictions[key]:\n",
    "        correct_label_in_top_n += 1\n",
    "        \n",
    "    else: \n",
    "        for j in diagnoses:\n",
    "            if j in diseases:\n",
    "                gene = omim_genes[j]\n",
    "                if gene == data['label'][key]:\n",
    "                    correct_label_in_top_n += 1\n",
    "\n",
    "print('Correct labels found in top 100:', correct_label_in_top_n)\n",
    "print('Accuracy:', 23/553 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
